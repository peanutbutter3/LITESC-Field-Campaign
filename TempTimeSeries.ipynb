{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before running, be sure to mount your drive and change the path to the path of your LITESC folder only (the code will navigate from there)\n",
        "\n",
        "Created by Paige Bartels\n"
      ],
      "metadata": {
        "id": "hFfAEPjmXRgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM0eN2WnVGka",
        "outputId": "90eb5768-0b71-4ec4-faf9-cc0aecc4551e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Mounting your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Buoy Import and Processing\n",
        "# Paige's path: path='/content/drive/MyDrive/Colab Notebooks/2024 LITESC Project Experimental Plan/Photos and Data/Buoy Data - Corrected CSV/'\n",
        "path='/content/drive/MyDrive/Colab Notebooks/2024 LITESC Project Experimental Plan/'\n",
        "fn='Photos and Data/Buoy Data - Corrected CSV/buoydata_45023.csv' # 45025 for L'Anse, 45023 for McLain\n",
        "ds=path+fn\n",
        "\n",
        "# See this link for description of values: https://www.ndbc.noaa.gov/faq/measdes.shtml#stdmet\n",
        "data=pd.read_csv(ds, header=1, skiprows=[2]) #Loading in data as pandas DataFrame\n",
        "data=data.replace(to_replace='MM',value='NaN') # Replacing \"MM\" values with NaN\n",
        "\n",
        "# Shifting time from EDT (UTC-4), accounting for day and month changes\n",
        "data['hh']=data['hh']+4\n",
        "data.loc[data['hh']>=24,'DD']=data['DD']+1\n",
        "data.loc[data['hh']>=24,'hh']=data['hh']-24\n",
        "data.loc[data['DD']>30,'MM']=data['MM']+1\n",
        "\n",
        "# Creating two-digit (padded) times\n",
        "data['hh']=data['hh'].astype('str')\n",
        "data['MM']=data['MM'].astype('str')\n",
        "data['mm']=data['mm'].astype('str')\n",
        "data['DD']=data['DD'].astype('str')\n",
        "\n",
        "data.loc[data['hh'].astype('int')<10,'hh']='0'+data['hh']\n",
        "data.loc[data['mm'].astype('int')<10,'mm']='0'+data['mm']\n",
        "data.loc[data['MM'].astype('int')<10,'MM']='0'+data['MM']\n",
        "data.loc[data['DD'].astype('int')<10,'DD']='0'+data['DD']\n",
        "\n",
        "# Adding datetime\n",
        "data['Date']=data['#YY'].astype('str')+data['MM']+data['DD']+' '+data['hh']+':'+data['mm']\n",
        "\n",
        "# Reversing data, so chronological\n",
        "data = data.iloc[::-1]\n",
        "data.reset_index(drop=True, inplace=True) # Resetting index of table\n",
        "\n",
        "if fn[-5]=='5': # L'Anse\n",
        "  # Selecting IOP days for L'Anse (45025)\n",
        "  iop_days=data.where(data['MM'].astype('int')==10).where(data['DD'].astype('int')>=14).where(data['DD'].astype('int')<=15)\n",
        "  iop_days=iop_days.dropna()\n",
        "  site=\"L'Anse\"\n",
        "  days='October 14-15, 2024'\n",
        "\n",
        "else:\n",
        "  # Selecting IOP days for McLain (45023)\n",
        "  iop_days=data.where(data['MM'].astype('int')==10).where(data['DD'].astype('int')>=12).where(data['DD'].astype('int')<=13)\n",
        "  iop_days=iop_days.dropna()\n",
        "  site='McLain'\n",
        "  days='October 12-13, 2024'\n",
        "\n",
        "if len(iop_days['mm'])%3!=0: # creating NaNs for missing data\n",
        "  errors=0\n",
        "  for i in iop_days.index:\n",
        "    check=i-iop_days.index[0]+errors\n",
        "    if i>iop_days.index[-1]-1:\n",
        "      break\n",
        "    if check==0 or check%3==0:\n",
        "      if iop_days['mm'][i]!='00':\n",
        "        iop_days.loc[(iop_days.index+1+errors)[0]]=[2024.0, 10, iop_days['DD'][i+1], iop_days['hh'][i+1],'00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,iop_days['Date'][i+1][0:12]+'00']\n",
        "        errors+=1\n",
        "    elif check%3==1:\n",
        "      if iop_days['mm'][i]!='20':\n",
        "        iop_days.loc[(iop_days.index+1+errors)[0]] = [2024.0, 10, iop_days['DD'][i-1], iop_days['hh'][i-1],'20',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,iop_days['Date'][i-1][0:12]+'20']\n",
        "        errors+=1\n",
        "    elif check%3==2:\n",
        "      if iop_days['mm'][i]!='40':\n",
        "        iop_days.loc[(iop_days.index+1+errors)[0]] = [2024.0, 10, iop_days['DD'][i-1], iop_days['hh'][i-1],'40',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN,iop_days['Date'][i-1][0:12]+'40']\n",
        "        errors+=1\n",
        "  iop_days.index = iop_days.index + errors  # shifting index\n",
        "  iop_days=iop_days.sort_values(by=['DD','hh','mm'])\n",
        "  iop_days.reset_index(drop=True, inplace=True) # Resetting index of table\n",
        "  iop_days=iop_days[1:]\n",
        "\n",
        "  print(f'Buoy Errors: {errors}')\n",
        "  print('Buoy read-in complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkcyU-FPWEGi",
        "outputId": "72a5beaa-b7a0-4c39-8f02-54d6137e03a8"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buoy Errors: 2\n",
            "Buoy read-in complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Num\n",
        "# Reading in HOBO data files\n",
        "filen='Photos and Data/HOBO_fixed/'\n",
        "file_list=glob.glob(path+filen+'*')\n",
        "file_list=file_list[:-1] # the last file is a .ipynb file (not a csv)\n",
        "\n",
        "# From HOBO .ipynb file\n",
        "# Loops to read in the data. Will be divided into two parts\n",
        "big_data_dict = {}\n",
        "\n",
        "for fileName in file_list:\n",
        "  data_read = pd.read_csv(fileName, header = 0, on_bad_lines='skip')\n",
        "  day=fileName.split('/')[-1].split('_')[0][-2:]\n",
        "  site=fileName.split('/')[-1].split('_')[1].split('.')[0]\n",
        "  big_data_dict[f\"10_{day}_24:{site}\"] = data_read\n",
        "\n",
        "# Code block to clean up the data\n",
        "for key in big_data_dict.keys():\n",
        "  big_data_dict[key][big_data_dict[key].columns[1]] = pd.to_datetime(big_data_dict[key][big_data_dict[key].columns[1]]) # converting to datetime\n",
        "  big_data_dict[key][big_data_dict[key].columns[1]] = big_data_dict[key][big_data_dict[key].columns[1]] + timedelta(hours=4) # conversion from EDT to UTC\n",
        "\n",
        "  data_only = big_data_dict[key][big_data_dict[key].columns[2:]]\n",
        "  data_only=data_only.replace(to_replace='-800',value=np.NaN)\n",
        "  big_data_dict[key][big_data_dict[key].columns[2:]] = data_only # converting all -800 values to nans\n",
        "\n",
        "  multiple=False\n",
        "  num=1440\n",
        "  if \"McLain\" in key or \"Lanse\" in key or \"L'Anse\" in key:\n",
        "    multiple=True\n",
        "    num=1440*2\n",
        "\n",
        "  print(key)\n",
        "\n",
        "  # Adding padding to be same resolution\n",
        "  # Creating Hours\n",
        "  hr=(np.ones(num))\n",
        "  hr=hr.astype('str')\n",
        "  for i in range(0,24):\n",
        "    h=str(i)\n",
        "    r='0'\n",
        "    if i>=10:\n",
        "      r=''\n",
        "    pad=i*60\n",
        "    hr[pad:60+pad]=f'{r}{h}'\n",
        "\n",
        "  if multiple:\n",
        "    for i in range(0,24):\n",
        "      h=str(i)\n",
        "      r='0'\n",
        "      if i>=10:\n",
        "        r=''\n",
        "      pad=i*60 + 1440\n",
        "      hr[pad:60+pad]=f'{r}{h}'\n",
        "\n",
        "  # Creating minutes\n",
        "  min=np.ones(num)\n",
        "  min=min.astype('str')\n",
        "  for i in range(0,24):\n",
        "    for j in range(0,60):\n",
        "      m=str(j)\n",
        "      n='0'\n",
        "      if j>=10:\n",
        "        n=''\n",
        "      pad=i*60\n",
        "      min[pad+j]=f'{n}{m}'\n",
        "  if multiple:\n",
        "    for i in range(0,24):\n",
        "      for j in range(0,60):\n",
        "        m=str(j)\n",
        "        n='0'\n",
        "        if j>=10:\n",
        "          n=''\n",
        "        pad=i*60 + 1440\n",
        "        min[pad+j]=f'{n}{m}'\n",
        "\n",
        "  # Finding the index of where data starts and ends in the new resolution\n",
        "  firstTime=big_data_dict[key]['TimeUTC'][0]\n",
        "  firsthr=firstTime.split(' ')[1].split(':')[0]\n",
        "  firstmin=firstTime.split(' ')[1].split(':')[1]\n",
        "  firstInd=int(firsthr)*60+int(firstmin)-1\n",
        "\n",
        "  numInd=len(big_data_dict[key])\n",
        "  lastInd=firstInd+numInd\n",
        "\n",
        "  endPad=(num-(lastInd+1))\n",
        "\n",
        "  # Cutting down to 24 hr UTC days\n",
        "  if lastInd>num:\n",
        "    i=lastInd-num\n",
        "    endPad=0\n",
        "    lastTime=big_data_dict[key]['TimeUTC'][len(big_data_dict[key])-(1+(lastInd-num))]\n",
        "    try:\n",
        "       lasthr=lastTime.split(' ')[1].split(':')[0]\n",
        "       lastmin=lastTime.split(' ')[1].split(':')[1]\n",
        "       lastInd=int(lasthr)*60+int(lastmin)-1\n",
        "       i=lastInd-num\n",
        "    except:\n",
        "      lastTime=big_data_dict[key]['TimeUTC'][len(big_data_dict[key])-(1+(lastInd-num-1))]\n",
        "      lasthr=lastTime.split(' ')[1].split(':')[0]\n",
        "      lastmin=lastTime.split(' ')[1].split(':')[1]\n",
        "      lastInd=int(lasthr)*60+int(lastmin)-1\n",
        "      i+=1\n",
        "    #print(i)\n",
        "    big_data_dict[key]=big_data_dict[key][:-i]\n",
        " #print(big_data_dict[key])\n",
        "\n",
        "  # Creating new dataframe to add padding\n",
        "  df = pd.DataFrame(columns = ['TimeUnix', 'TimeUTC','SolarRadiation','Temperature','DewPoint','RelativeHumidity','WindDirection','WindSpeed','GustSpeed','Pressure','HH','MM'])\n",
        "  for col in df.columns.tolist():\n",
        "    if col=='HH':\n",
        "      df[col]=hr\n",
        "    elif col=='MM':\n",
        "      df[col]=min\n",
        "    else:\n",
        "      df[col]= (np.full(firstInd+1, np.nan)).tolist()+big_data_dict[key][col].tolist()+(np.full(endPad,np.nan)).tolist()\n",
        "\n",
        "  big_data_dict[key]=df\n",
        "  #print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZHlPmAlYJp8",
        "outputId": "e34f2c3d-9500-41c9-c024-60badce63e3c"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10_13_24:McLain\n",
            "10_12_24:Lily\n",
            "10_13_24:Swedetown\n",
            "10_12_24:Swedetown\n",
            "10_15_24:Lanse\n",
            "10_14_24:Beach\n",
            "10_14_24:Cliff\n",
            "10_15_24:Beach\n",
            "10_15_24:Cliff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining Data into Plot\n",
        "keys=[]\n",
        "size=1.5\n",
        "colors=['#d55e00','#cc79a7','#0072b2','#f0e442','#009e73']\n",
        "if fn[-5]=='5': # L'Anse\n",
        "  day=14\n",
        "  site=\"L'Anse\"\n",
        "  site1='Lanse'\n",
        "  site2='Cliff'\n",
        "  site3='Beach'\n",
        "  for key in big_data_dict.keys():\n",
        "    if key.split(':')[1]==site1 or key.split(':')[1]==site2 or key.split(':')[1]==site3:\n",
        "      keys.append(key)\n",
        "\n",
        "else:\n",
        "  day=12\n",
        "  site='McLain'\n",
        "  site1=site\n",
        "  site2='Lily'\n",
        "  site3='Swedetown'\n",
        "  for key in big_data_dict.keys():\n",
        "    if key.split(':')[1]==site1 or key.split(':')[1]==site2 or key.split(':')[1]==site3:\n",
        "      keys.append(key)\n",
        "\n",
        "for i in range(0,2):\n",
        "  # Buoy Data\n",
        "  newData=iop_days.where(iop_days['DD'].astype('int')==day)\n",
        "  if day==14 or day==12:\n",
        "    newData=newData[:68]\n",
        "  else:\n",
        "    newData=newData[-69:]\n",
        "\n",
        "  fig=plt.figure(dpi=150,figsize=(10,5))\n",
        "  ax=plt.axes()\n",
        "\n",
        "  # HOBO Data\n",
        "  hobos={}\n",
        "  k=0\n",
        "  for key in keys:\n",
        "    hobo=big_data_dict[key]\n",
        "    dates=hobo['TimeUTC']\n",
        "    sites=key.split(':')[1]\n",
        "    day=int(day)\n",
        "\n",
        "    if sites!='McLain' and sites!='Lanse' and key.split('_')[1]!=str(day):\n",
        "      continue\n",
        "\n",
        "    if sites==\"McLain\" or sites=='Lanse':\n",
        "      if day==14 or day==12:\n",
        "        hobo=hobo[:1440]\n",
        "      else:\n",
        "        hobo=hobo[1440:]\n",
        "\n",
        "    dates=hobo['TimeUTC']\n",
        "    time=(hobo['HH']+':'+hobo['MM'])\n",
        "    if sites=='Lanse':\n",
        "      sites=\"L'Anse\"\n",
        "    elif sites=='Beach':\n",
        "      sites='Second Sand Beach'\n",
        "    elif sites=='Cliff':\n",
        "      sites='Baraga Cliffs'\n",
        "    elif sites=='Lily':\n",
        "      sites='Lily Pond'\n",
        "    ax.plot(time,hobo['Temperature'].astype('float64'),label=sites,linewidth=size,c=colors[k])\n",
        "    k+=1\n",
        "\n",
        "  buoyTime=time.tolist()[::20]\n",
        "  buoyTime=buoyTime[3:-1]\n",
        "\n",
        "  try:\n",
        "    ax.plot(buoyTime,newData['WTMP'].astype('float64'),label='Bouy Lake',linewidth=size+0.5,c='gold')\n",
        "    ax.plot(buoyTime,newData['ATMP'].astype('float64'),label='Buoy Air',linewidth=size+0.5,c=colors[4])\n",
        "  except:\n",
        "    print(len(buoyTime))\n",
        "    print(len(newData['WTMP']))\n",
        "    ax.plot(buoyTime,newData['WTMP'][:-1].astype('float64'),label='Bouy Lake',linewidth=size+0.5,c='gold')\n",
        "    ax.plot(buoyTime,newData['ATMP'][:-1].astype('float64'),label='Buoy Air',linewidth=size+0.5,c=colors[4])\n",
        "\n",
        "  zoom=False\n",
        "  if zoom:\n",
        "    skip=60\n",
        "  else:\n",
        "    skip=180\n",
        "  ax.set_title(f\"October {day}, 2024\")\n",
        "  xlabels=((hobo['HH'].astype('str')+':'+hobo['MM'].astype('str'))[::skip])\n",
        "  ax.set_xticks(np.arange(0,1440,skip),labels=xlabels, rotation=45)\n",
        "  plt.suptitle(f\"{site} HOBO and Buoy Temperatures\")\n",
        "  ax.set_ylabel('Temperature (degC)')\n",
        "  ax.set_xlabel('UTC Time (EDT+4)')\n",
        "  pos = ax.get_position()\n",
        "  ax.set_position([pos.x0, pos.y0, pos.width * 0.9, pos.height])\n",
        "  ax.legend(loc='center right', bbox_to_anchor=(1.3, 0.9))\n",
        "  if zoom:\n",
        "    ax.set_xlim('14:00','23:00')\n",
        "  #plt.show()\n",
        "  plt.close()\n",
        "  day+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHcA_ONSfR55",
        "outputId": "5a39f2cf-9990-41be-c283-f7e2037807cb"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68\n",
            "69\n"
          ]
        }
      ]
    }
  ]
}